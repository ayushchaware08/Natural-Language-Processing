{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcyGMix8BIZI0quzlGTNEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushchaware08/Natural-Language-Processing/blob/main/NLP_P7_NGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name:Ayush Chaware**\n",
        "\n",
        "**Roll No: 22**\n",
        "\n",
        "Aim: Perform NGram Language Modelling using NLTK"
      ],
      "metadata": {
        "id": "Ko1oP83pEPzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "from nltk.corpus import reuters\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "SuXW-2biEPaX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZSPn7tfLCgV",
        "outputId": "f43fcbe2-4e7b-4dab-ad4d-bf1998bace55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "words = nltk.word_tokenize(' '.join(reuters.words()))\n"
      ],
      "metadata": {
        "id": "KFQUFOUwLHdF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trigrams\n",
        "tri_grams = list(trigrams(words))\n"
      ],
      "metadata": {
        "id": "C-P3UHTaMDPN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build a trigram model\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n"
      ],
      "metadata": {
        "id": "GcAXRNEEME8c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count frequency of co-occurrence\n",
        "for w1, w2, w3 in tri_grams:\n",
        "    model[(w1, w2)][w3] += 1\n"
      ],
      "metadata": {
        "id": "1g7VogPBMGO8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the counts into probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n"
      ],
      "metadata": {
        "id": "3fYkhOuvMHd8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the next word\n",
        "def predict_next_word(w1, w2):\n",
        "  #  W1 WORD1 w2 WORD2 predetion of 3rd word\n",
        "    next_word = model[w1, w2]\n",
        "    if next_word:\n",
        "        predicted_word = max(next_word, key=next_word.get)  # Choose the most likely next word\n",
        "        return predicted_word\n",
        "    else:\n",
        "        return \"No prediction available\"\n"
      ],
      "metadata": {
        "id": "YYhXAKvbMI39"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "print(\"Next Word:\", predict_next_word('the', 'morning'))"
      ],
      "metadata": {
        "id": "dwCjK2puMYC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc8b7f9-9fb1-48ff-f4cd-0cadb4334200"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next Word: session\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwkHR_fHSDZb"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}