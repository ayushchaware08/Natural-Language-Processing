{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl4XadXvBxDie/3SpTfz8g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentiment Analysis**"
      ],
      "metadata": {
        "id": "IDCjHmPnTHHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Twitter data -> Data PReprocessing -> Train Test Split -> Logistic Regression  Model"
      ],
      "metadata": {
        "id": "VDnD2mubUikV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trained Logistic Model**\n",
        "\n",
        "New Data -> Logistic Regression Model -> Positive Tweert (1) / Negative Tweet (0)"
      ],
      "metadata": {
        "id": "YC9GXC-GUw4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Dependencies"
      ],
      "metadata": {
        "id": "3Gjtkz7FaY5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjtsy9N0bU5j",
        "outputId": "0bd16837-c8e1-4b68-c1d7-3b99487bd825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "pfF4Am-kcq3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dHobUH5hmIq",
        "outputId": "4711a4fd-8fc6-42d7-95c1-bb139745d388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# basic  libraries\n",
        "# for vector manupulation\n",
        "import numpy as np\n",
        "\n",
        "# for processing and analysis of data frame (structured data)\n",
        "import pandas as pd\n",
        "\n",
        "# for patter matching serch through data (re = regular expression)\n",
        "import re\n",
        "\n",
        "# natural language toolkit corpus is the module for stopword\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for stemming (root word)\n",
        "# from nltk.stem.porter import Porterstemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# we cant feed textual data into our machine convert into machine code\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# dataset = Train Data + Test Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# we are training data using  logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# my acurracy score\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "oYWaEHqtXICs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnVa-XJbSrip"
      },
      "outputs": [],
      "source": [
        "# Fatching the Dataset\n",
        "dataset = '/content/Twitter data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq50bOPPXgJN",
        "outputId": "8bb0e013-5891-4529-f06c-f15797a9fc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing Stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB9QVfPDf5W4",
        "outputId": "fd5b3759-f777-4363-c5fe-bfcad0cfe665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data** **Preprocessing**"
      ],
      "metadata": {
        "id": "IbBDtQ9qgNw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data form csv to pandas dataframe\n",
        "# we have to gave encoding normal standard\n",
        "twitter_data = pd.read_csv('/content/Twitter data.csv', encoding='utf-8', )\n"
      ],
      "metadata": {
        "id": "wwizCLtDgLzu",
        "outputId": "1fdc36a0-1908-4cda-b917-f1bcded7c876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Twitter data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6f58fa2f0cde>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading data form csv to pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# we have to gave encoding normal standard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtwitter_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Twitter data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Twitter data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking number of rows and col\n",
        "twitter_data.shape"
      ],
      "metadata": {
        "id": "9X-tFdXBg3Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first five\n",
        "twitter_data.head()\n"
      ],
      "metadata": {
        "id": "GRgrN2F1hGrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data.tail()"
      ],
      "metadata": {
        "id": "8hB3AJ0MDjHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need to make 1st row as title row\n",
        "column_names =['target', 'id', 'date', 'flag', 'user','text']\n",
        "twitter_data = pd.read_csv('/content/Twitter data.csv', names=column_names, encoding = 'ISO-8859-1')"
      ],
      "metadata": {
        "id": "0fOX_I7hiVzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data.head()"
      ],
      "metadata": {
        "id": "dE368ZmBjA_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here target is 0 = negative, 2 = neutral, 4 = positive"
      ],
      "metadata": {
        "id": "xPWebfj_jPSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# counting the null values\n",
        "twitter_data.isnull().sum()"
      ],
      "metadata": {
        "id": "j-Ol7zjsjFvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing null values\n",
        "twitter_data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "ATp_F0h6oW0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the distrubution of target table (no of labels)\n",
        "twitter_data['target'].value_counts()"
      ],
      "metadata": {
        "id": "H0pEAUerjt3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "target convert \"4\" to \"1\""
      ],
      "metadata": {
        "id": "M0WoFjjPpkOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data.replace({'target': {4: 1}}, inplace=True)"
      ],
      "metadata": {
        "id": "pGiOfvE4pj6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking\n",
        "twitter_data['target'].value_counts()"
      ],
      "metadata": {
        "id": "m28rlwUtkkfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0 -> negative   1 -> positive**"
      ],
      "metadata": {
        "id": "1EbzdHThqXcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "y2s-A8xatlvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #reduce it to root word to make it easier\n",
        "port_stem = PorterStemmer()"
      ],
      "metadata": {
        "id": "VUe_SJp8qR1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **most imp part**\n",
        "\n",
        "Function\n",
        "\n",
        "text col -> remove 124256@!#$% onlu a-z A-Z-> apply lowercase to all -> remove stop word -> join"
      ],
      "metadata": {
        "id": "md9MCW66tnrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# passing text column\n",
        "\n",
        "def stemming(content):\n",
        "  # for patter matching serch through data (re = regular expression)\n",
        "  stemmed_content = re.sub('[^a-zA-Z]',' ',content)    # all alphabet only remove ,.@\n",
        "  stemmed_content = stemmed_content.lower()    # only lowercase\n",
        "  stemmed_content = stemmed_content.split()   # split word and list\n",
        "  # checking if the word is present in stem content  and not in stopword\n",
        "  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
        "  # join all\n",
        "  stemmed_content = ' '.join(stemmed_content)\n",
        "\n",
        "  return stemmed_content"
      ],
      "metadata": {
        "id": "kBKlE5MIqWFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new column for semmed data\n",
        "twitter_data['stemmed_content'] = twitter_data['text'].apply(stemming)    # most time consuming as large dataset"
      ],
      "metadata": {
        "id": "j4s5KEauuijf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first 5 row\n",
        "twitter_data.head()"
      ],
      "metadata": {
        "id": "Cu_o5zwyvIeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print col Stemmed Content\n",
        "print(twitter_data['stemmed_content'])"
      ],
      "metadata": {
        "id": "b6myzO_BvuSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print Col target\n",
        "print(twitter_data['target'])"
      ],
      "metadata": {
        "id": "osELbDUav0vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now thw model is trying to understand what is wrong and what is right."
      ],
      "metadata": {
        "id": "f0yxPZrrwC8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i/p : data (tweet)\n",
        "\n",
        "o/p (lebel) : 0(negative or 1 (positive)"
      ],
      "metadata": {
        "id": "zcmCTeBBwUEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seperating the data and label\n",
        "X = twitter_data['stemmed_content'].values\n",
        "Y = twitter_data['target'].values"
      ],
      "metadata": {
        "id": "p3hrUmSewCUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "Ua97qCvAw1M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "id": "wDUaztMrw9V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting into train and test Data set**"
      ],
      "metadata": {
        "id": "3jhfwFiJxC4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify = Y , random_state = 2)\n",
        "\n",
        "# test_size 20% of data   train data = 80%  ((student and exam in school))\n",
        "# stratify = i want equal distribution of 0 and 1 i.e 1:1 ratio\n",
        "# random state makes sure data split same in me as well as you"
      ],
      "metadata": {
        "id": "HvCUffjrw_PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape,X_train.shape,X_test.shape)"
      ],
      "metadata": {
        "id": "9j4lxD8-xrLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape,Y_train.shape,Y_test.shape)"
      ],
      "metadata": {
        "id": "y0jw5d80IKnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "id": "1Fc-KNFr1E3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML model doesnt understant the language so need to convet into numeracal value\n",
        "method called as **feature extraction**"
      ],
      "metadata": {
        "id": "rFdntWgZ1Wcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting Textual data into Vector Data\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "# fitting means  understing the nature of data feeded\n",
        "# transform = text to num"
      ],
      "metadata": {
        "id": "YDh9lEin1S6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "all the tweets are converted into important value by the TF ID vectorizer\n",
        "\n",
        "that perticular word is positive or negative\n",
        "\n",
        "this tf Id vectorizer used to  assign positive or negative"
      ],
      "metadata": {
        "id": "HGCP5LiV8z7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "id": "GwH9GcC49ltw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "id": "VfYpz9qE16gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DP and train test split done\n",
        "\n",
        "# Training the ML model"
      ],
      "metadata": {
        "id": "iieSGHlY-xTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOgistic Regression\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)"
      ],
      "metadata": {
        "id": "190iyhmY-5e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train)\n",
        "# model get to know what is positive and negative means"
      ],
      "metadata": {
        "id": "e_1JZeHp_JET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "(Training Accuracy)"
      ],
      "metadata": {
        "id": "Ksvl6tds_g6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy score on the training data\n",
        "\n",
        "# making model to predict\n",
        "# X_train_prediction = model.predict(X_train)\n",
        "\n",
        "# comparing the true labels with predicted value\n",
        "# training_data_accuracy = accuracy_score(Y_train,X_train_prediction)"
      ],
      "metadata": {
        "id": "H7oKKNtu_QjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Accuracy Score of the training data\",training_data_accuracy)"
      ],
      "metadata": {
        "id": "hetQtsslA71N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Accuracy\n",
        "(Test Accuracy)"
      ],
      "metadata": {
        "id": "sCcRqQBYBdxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " X_test_prediction = model.predict(X_test)\n",
        " testing_data_accuracy = accuracy_score(Y_test,X_test_prediction)"
      ],
      "metadata": {
        "id": "Ik8_ktwnBMPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy Score of the Testing data\",testing_data_accuracy)"
      ],
      "metadata": {
        "id": "VWrvGw_PCGh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trainig data : 84&\n",
        "\n",
        "testing data : 76%\n",
        "\n",
        "very close to each other  therform model has performed well\n",
        "Overfitting : where traiining acc > test accuracy that means the model hasn't learned properly\n",
        "\n",
        "yhats why it is importtant to test dataset"
      ],
      "metadata": {
        "id": "-3WrNhLwCa0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "nSw59Xj8KHe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "7prb3lexCVvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'Trained_model.sav' # save file\n",
        "pickle.dump(model,open(filename,'wb'))  # save model -> open -> read"
      ],
      "metadata": {
        "id": "RXEF2e4eKK1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#using saved model\n",
        "for future prediction\n"
      ],
      "metadata": {
        "id": "z_09wW9fLks0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the saved model\n",
        "loaded_model = pickle.load(open('/content/Trained_model.sav','rb'))  #rb - reading"
      ],
      "metadata": {
        "id": "mFMbfERdLPvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X_test[200]  # 200th data point and o/p data is in y test\n",
        "print(Y_test[200])\n",
        "\n",
        "# predictions store\n",
        "prediction = loaded_model.predict(X_new)\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "id": "MaVb0rgRL2Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (prediction[0] == 0):\n",
        "  print(\"Negative Tweet\")\n",
        "\n",
        "else:\n",
        "  print(\"Positive Tweet\")"
      ],
      "metadata": {
        "id": "Hs-q1dr8MqIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other uses\n",
        "\n",
        "\n",
        "1. Sentence analysis\n",
        "2. Amazon comment section\n",
        "3. user review of the peoduct (\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kv4cJhgRQgKj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2SAyosnCM9LU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}